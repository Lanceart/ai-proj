{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Feature Selection Algorithm:\n",
      "Type in the name of the file to test: new_data.txt\n",
      "\n",
      "Type the algorithm you want to run:\n",
      " \n",
      " 1.Forward Selection\n",
      " 2.Backward Elimination\n",
      " \n",
      "1\n",
      "\n",
      "This dataset has 10 features (no including the class attribute), with 1000 instances\n",
      "Beginning search.\n",
      "\n",
      "\n",
      " On level 1 of the search tree contains []\n",
      "Using feature(s) [1] accuracy is 50.550550550550554 %\n",
      "Using feature(s) [2] accuracy is 58.85885885885885 %\n",
      "Using feature(s) [3] accuracy is 65.36536536536536 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "\n",
      " On level 1 of the search tree, feature 3 was added to the current set\n",
      "\n",
      " With  1  features, the accuracy is:  65.36536536536536 %\n",
      "\n",
      " On level 2 of the search tree contains [3]\n",
      "Using feature(s) [3, 1] accuracy is 63.263263263263255 %\n",
      "Using feature(s) [3, 2] accuracy is 66.36636636636636 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s) [3, 5] accuracy is 66.96696696696696 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "\n",
      " On level 2 of the search tree, feature 5 was added to the current set\n",
      "\n",
      " With  2  features, the accuracy is:  66.96696696696696 %\n",
      "\n",
      " On level 3 of the search tree contains [3, 5]\n",
      "Using feature(s) [3, 5, 1] accuracy is 67.46746746746747 %\n",
      "Using feature(s) [3, 5, 2] accuracy is 69.66966966966966 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "\n",
      " On level 3 of the search tree, feature 2 was added to the current set\n",
      "\n",
      " With  3  features, the accuracy is:  69.66966966966966 %\n",
      "\n",
      " On level 4 of the search tree contains [3, 5, 2]\n",
      "Using feature(s) [3, 5, 2, 1] accuracy is 70.87087087087087 %\n",
      "Using feature(s) [3, 5, 2, 4] accuracy is 72.27227227227228 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "\n",
      " On level 4 of the search tree, feature 4 was added to the current set\n",
      "\n",
      " With  4  features, the accuracy is:  72.27227227227228 %\n",
      "\n",
      " On level 5 of the search tree contains [3, 5, 2, 4]\n",
      "Using feature(s) [3, 5, 2, 4, 1] accuracy is 71.47147147147147 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s) [3, 5, 2, 4, 7] accuracy is 73.77377377377378 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "\n",
      " On level 5 of the search tree, feature 7 was added to the current set\n",
      "\n",
      " With  5  features, the accuracy is:  73.77377377377378 %\n",
      "\n",
      " On level 6 of the search tree contains [3, 5, 2, 4, 7]\n",
      "Using feature(s) [3, 5, 2, 4, 7, 1] accuracy is 72.07207207207207 %\n",
      "Using feature(s) [3, 5, 2, 4, 7, 6] accuracy is 72.17217217217218 %\n",
      "Using feature(s) [3, 5, 2, 4, 7, 8] accuracy is 74.57457457457457 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s) [3, 5, 2, 4, 7, 10] accuracy is 74.77477477477478 %\n",
      "\n",
      " On level 6 of the search tree, feature 10 was added to the current set\n",
      "\n",
      " With  6  features, the accuracy is:  74.77477477477478 %\n",
      "\n",
      " On level 7 of the search tree contains [3, 5, 2, 4, 7, 10]\n",
      "Using feature(s) [3, 5, 2, 4, 7, 10, 1] accuracy is 72.67267267267268 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s) [3, 5, 2, 4, 7, 10, 8] accuracy is 74.87487487487488 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "\n",
      " On level 7 of the search tree, feature 8 was added to the current set\n",
      "\n",
      " With  7  features, the accuracy is:  74.87487487487488 %\n",
      "\n",
      " On level 8 of the search tree contains [3, 5, 2, 4, 7, 10, 8]\n",
      "Using feature(s) [3, 5, 2, 4, 7, 10, 8, 1] accuracy is 74.37437437437437 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "\n",
      " On level 8 of the search tree, feature 1 was added to the current set\n",
      "\n",
      " With  8  features, the accuracy is:  74.37437437437437 %\n",
      "\n",
      " On level 9 of the search tree contains [3, 5, 2, 4, 7, 10, 8, 1]\n",
      "Using feature(s) [3, 5, 2, 4, 7, 10, 8, 1, 6] accuracy is 74.17417417417418 %\n",
      "Using feature(s) [3, 5, 2, 4, 7, 10, 8, 1, 9] accuracy is 74.77477477477478 %\n",
      "\n",
      " On level 9 of the search tree, feature 9 was added to the current set\n",
      "\n",
      " With  9  features, the accuracy is:  74.77477477477478 %\n",
      "\n",
      " On level 10 of the search tree contains [3, 5, 2, 4, 7, 10, 8, 1, 9]\n",
      "Using feature(s) [3, 5, 2, 4, 7, 10, 8, 1, 9, 6] accuracy is 73.57357357357357 %\n",
      "\n",
      " On level 10 of the search tree, feature 6 was added to the current set\n",
      "\n",
      " With  10  features, the accuracy is:  73.57357357357357 %\n",
      "\n",
      "Finish search!! The best feature subset is: [3, 5, 2, 4, 7, 10, 8] which has an accuracy of 74.87487487487488 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def Slide_window(my_window,layers):#Stop instead of useless counting\n",
    "    return (my_window[layers-2] > my_window[layers-1]) and (my_window[layers-1] > my_window[layers])\n",
    "\n",
    "#Normalizing data \n",
    "def Normalize_data(data):\n",
    "    data_nor = stats.zscore(data)\n",
    "    return data_nor\n",
    "\n",
    "#Performing Leave One Out Cross Validation and minimax like pruning\n",
    "def CrossValidation_ML(features_in,data_nor,select_f, p, tolerate_error):\n",
    "    error_counter = 0\n",
    "    feature=list(features_in)\n",
    "    if p==1:\n",
    "        feature.append(select_f)\n",
    "    if p ==2:\n",
    "        feature.remove(select_f)\n",
    "    count = 0\n",
    "    dis = np.inf\n",
    "    final=0\n",
    "    for i in range(0,len(data_nor)):\n",
    "        dis = np.inf\n",
    "        for k in range(0,len(data_nor)):\n",
    "            if not np.array_equal(k,i):\n",
    "                ss = 0\n",
    "                for j in range(0, len(feature)):#Calculating Euclidean Distance to determine the nearest neighbors\n",
    "                    ss=ss+pow((data_nor[k][feature[j]] - data_nor[i][feature[j]]),2)\n",
    "                    d=np.sqrt(ss)\n",
    "                if d < dis:\n",
    "                    dis = d\n",
    "                    final = k\n",
    "        if (data_nor[final][0]==data_nor[i][0]):\n",
    "            count += 1\n",
    "            accuracy = (count / (len(data_nor)-1))#Calculating accuracy\n",
    "        else:\n",
    "            error_counter += 1\n",
    "        \n",
    "        if tolerate_error < error_counter:\n",
    "            print(\"Using feature(s), the feature has been stop by minimax like pruning\")\n",
    "            return 0\n",
    "    print(\"Using feature(s)\",feature, \"accuracy is\", accuracy*100, \"%\")\n",
    "    return accuracy\n",
    "\n",
    "#Performing Leave One Out Cross Validation\n",
    "def CrossValidation(features_in,data_nor,select_f, p):\n",
    "    feature=list(features_in)\n",
    "    if p==1:\n",
    "        feature.append(select_f)\n",
    "    if p ==2:\n",
    "        feature.remove(select_f)\n",
    "    count = 0\n",
    "    dis = np.inf\n",
    "    final=0\n",
    "    for i in range(0,len(data_nor)):\n",
    "        dis = np.inf\n",
    "        for k in range(0,len(data_nor)):\n",
    "            if not np.array_equal(k,i):\n",
    "                ss = 0\n",
    "                for j in range(0, len(feature)):#Calculating Euclidean Distance to determine the nearest neighbors\n",
    "                    ss=ss+pow((data_nor[k][feature[j]] - data_nor[i][feature[j]]),2)\n",
    "                    d=np.sqrt(ss)\n",
    "                if d < dis:\n",
    "                    dis = d\n",
    "                    final = k\n",
    "        if (data_nor[final][0]==data_nor[i][0]):\n",
    "            count += 1\n",
    "            accuracy = (count / (len(data_nor)-1))#Calculating accuracy\n",
    "    print(\"Using feature(s)\",feature, \"accuracy is\", accuracy*100, \"%\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def ForwardSelection(data_nor,NF):\n",
    "    print(\"Beginning search.\\n\")\n",
    "    current_features = []\n",
    "    final_acc = 0\n",
    "    best_feature=[]\n",
    "    my_windows = [0 for _ in range(NF)]\n",
    "    counter =  0\n",
    "    for i in range(1, NF+1):\n",
    "        print(\"\\n On level %d of the search tree\" % (i),\"contains\", current_features)\n",
    "        feature_select = 0\n",
    "        cur_acc=0.0\n",
    "        for j in range(1, NF+1):\n",
    "            if j not in current_features:\n",
    "                if j == 1:\n",
    "                    acc = CrossValidation(current_features,data_nor,j,1)\n",
    "                else:\n",
    "                    acc = CrossValidation_ML(current_features,data_nor,j,1,(int) ((1-cur_acc) * len(data_nor) ))\n",
    "                if acc> cur_acc:\n",
    "                    cur_acc = acc\n",
    "                    feature_select = j\n",
    "        \n",
    "        current_features.append(feature_select)\n",
    "        print(\"\\n On level %d of the search tree,\" % (i),\"feature %d was added to the current set\" % (feature_select))\n",
    "        print(\"\\n With \", len(current_features), \" features, the accuracy is: \", cur_acc * 100, \"%\")\n",
    "        \n",
    "        my_windows[counter] = cur_acc        \n",
    "        if counter > 2 and Slide_window(my_windows, counter):\n",
    "            break\n",
    "        counter += 1\n",
    "        if cur_acc >= final_acc: \n",
    "            final_acc= cur_acc\n",
    "            best_feature = list(current_features)\n",
    "\n",
    "    print()\n",
    "    print(\"Finish search!! The best feature subset is:\", best_feature,\"which has an accuracy of\", final_acc * 100, \"%\")\n",
    "\n",
    "\n",
    "def BackwardElimination(data_nor,NF):\n",
    "    param_stop= input(\"\\nType the accuracy param to early abandon(0-100): \\n You may input 100 if you don't want early abondon \\n \\n\")\n",
    "    param_stop= int(param_stop)\n",
    "    \n",
    "    print(\"Beginning search.\\n\")\n",
    "    final_acc = 0\n",
    "    best_feature=[]\n",
    "    current_features = [i for i in range(1, NF+1)]\n",
    "\n",
    "    for i in range(1, NF):\n",
    "        print(\"\\n On level %d of the search tree\" % (i),\"contains\", current_features)\n",
    "        feature_select = 0\n",
    "        cur_acc = 0\n",
    "        for j in range(1,NF):\n",
    "            if (j in current_features):\n",
    "                acc = CrossValidation(current_features,data_nor,j,2)\n",
    "                if acc > cur_acc:\n",
    "                    cur_acc = acc\n",
    "                    feature_select = j\n",
    "        if feature_select in current_features: \n",
    "            current_features.remove(feature_select) \n",
    "            print(\"\\n On level \", i, \" feature \", feature_select, \" was removed from the current set\")\n",
    "            print(\"\\n With \", len(current_features), \" features, the accuracy is: \", cur_acc * 100, \"%\")\n",
    "        if cur_acc >= final_acc: \n",
    "            final_acc = cur_acc\n",
    "            best_feature= list(current_features)\n",
    "        if final_acc * 100 > param_stop:\n",
    "            print()\n",
    "            print(\"Early abandon!! The best feature subset is:\", best_feature,\"which has an accuracy of\", final_acc * 100, \"%\")\n",
    "            return \n",
    "\n",
    "    print()\n",
    "    print(\"Finish search!! The best feature subset is:\", best_feature,\"which has an accuracy of\", final_acc * 100, \"%\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Welcome to the Feature Selection Algorithm:\")\n",
    "    files = input(\"Type in the name of the file to test: \")\n",
    "    \n",
    "    algorithm=input(\"\\nType the algorithm you want to run:\\n \\n 1.Forward Selection\\n 2.Backward Elimination\\n \\n\")\n",
    "   \n",
    "    data=np.loadtxt(files)\n",
    "#     print(data[1])\n",
    "#     data = data[1000:]\n",
    "    \n",
    "\n",
    "    data = data[:(int)(len(data)*0.9)]#sample the data\n",
    "    N=len(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(N)\n",
    "    data_normal = Normalize_data(data)\n",
    "    NF= len(data_normal[0])-1\n",
    "    print (\"\\nThis dataset has \"+ str(NF)+ \" features (no including the class attribute), with \"+str(N)+ \" instances\")\n",
    "   \n",
    "    if (algorithm == \"1\"):\n",
    "        ForwardSelection(data_normal,NF)\n",
    "    elif(algorithm == \"2\"):\n",
    "        BackwardElimination(data_normal,NF)\n",
    "    else:\n",
    "        print(\"Error input!\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type in the name of the file to test: knn_data\n",
      "nan\n",
      "[[1.63667061e+03 8.17988525e+02 2.56599519e+03 ... 1.49487863e+03\n",
      "  8.45136088e+02 0.00000000e+00]\n",
      " [1.01340276e+03 5.77587332e+02 2.64414127e+03 ... 1.19303252e+03\n",
      "  8.61081809e+02 1.00000000e+00]\n",
      " [1.30003550e+03 8.20518697e+02 2.02585447e+03 ... 1.96836751e+03\n",
      "  1.64718629e+03 1.00000000e+00]\n",
      " ...\n",
      " [9.21994822e+02 6.07996901e+02 2.06548253e+03 ... 9.78340107e+02\n",
      "  1.94330491e+03 1.00000000e+00]\n",
      " [1.15706935e+03 6.02749160e+02 1.54881000e+03 ... 1.26481808e+03\n",
      "  1.33187902e+03 1.00000000e+00]\n",
      " [1.28715003e+03 1.30360009e+03 2.24728754e+03 ... 8.46167511e+02\n",
      "  9.52895751e+02 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "# script for \n",
    "files = input(\"Type in the name of the file to test: \")\n",
    "my_data = genfromtxt(files, delimiter=',')\n",
    "print(my_data[0][0])\n",
    "my_data = np.delete(my_data,0,axis= 0)\n",
    "\n",
    "print(my_data)\n",
    "\n",
    "last_column = my_data[:, -1].reshape(-1, 1) # convert into vector\n",
    "# delete the last\n",
    "other_columns = my_data[:, :-1]\n",
    "# put them into together\n",
    "new_data = np.hstack((last_column, other_columns))\n",
    "np.savetxt('new_data.txt',new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
