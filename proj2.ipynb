{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Feature Selection Algorithm:\n",
      "Type in the name of the file to test: CS170_small_Data__31.txt\n",
      "\n",
      "Type the algorithm you want to run:\n",
      " \n",
      " 1.Forward Selection\n",
      " 2.Backward Elimination\n",
      " \n",
      "1\n",
      "\n",
      "This dataset has 10 features (no including the class attribute), with 900 instances\n",
      "Beginning search.\n",
      "\n",
      "\n",
      " On level 1 of the search tree contains []\n",
      "Using feature(s) [1] accuracy is 85.76 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "\n",
      " On level 1 of the search tree, feature 1 was added to the current set\n",
      "\n",
      " With  1  features, the accuracy is:  85.76 %\n",
      "\n",
      " On level 2 of the search tree contains [1]\n",
      "Using feature(s) [1, 2] accuracy is 83.2 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s) [1, 4] accuracy is 84.87 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s) [1, 6] accuracy is 98.22 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "\n",
      " On level 2 of the search tree, feature 6 was added to the current set\n",
      "\n",
      " With  2  features, the accuracy is:  98.22 %\n",
      "\n",
      " On level 3 of the search tree contains [1, 6]\n",
      "Using feature(s) [1, 6, 2] accuracy is 94.77 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s) [1, 6, 4] accuracy is 94.99 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s) [1, 6, 8] accuracy is 95.11 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "\n",
      " On level 3 of the search tree, feature 8 was added to the current set\n",
      "\n",
      " With  3  features, the accuracy is:  95.11 %\n",
      "\n",
      " On level 4 of the search tree contains [1, 6, 8]\n",
      "Using feature(s) [1, 6, 8, 2] accuracy is 90.1 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s) [1, 6, 8, 4] accuracy is 90.66 %\n",
      "Using feature(s) [1, 6, 8, 5] accuracy is 91.21 %\n",
      "Using feature(s) [1, 6, 8, 7] accuracy is 91.77 %\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "Using feature(s), the feature has been stop by minimax like pruning\n",
      "\n",
      " On level 4 of the search tree, feature 7 was added to the current set\n",
      "\n",
      " With  4  features, the accuracy is:  91.77 %\n",
      "\n",
      "Finish search!! The best feature subset is: [1, 6] which has an accuracy of 98.22 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def Slide_window(my_window,layers):#Stop instead of useless counting\n",
    "    return (my_window[layers-2] > my_window[layers-1]) and (my_window[layers-1] > my_window[layers])\n",
    "\n",
    "#Normalizing data \n",
    "def Normalize_data(data):\n",
    "    data_nor = stats.zscore(data)\n",
    "    return data_nor\n",
    "\n",
    "#Performing Leave One Out Cross Validation and minimax like pruning\n",
    "def CrossValidation_ML(features_in,data_nor,select_f, p, tolerate_error):\n",
    "    error_counter = 0\n",
    "    feature=list(features_in)\n",
    "    if p==1:\n",
    "        feature.append(select_f)\n",
    "    if p ==2:\n",
    "        feature.remove(select_f)\n",
    "    count = 0\n",
    "    dis = np.inf\n",
    "    final=0\n",
    "    for i in range(0,len(data_nor)):\n",
    "        dis = np.inf\n",
    "        for k in range(0,len(data_nor)):\n",
    "            if not np.array_equal(k,i):\n",
    "                ss = 0\n",
    "                for j in range(0, len(feature)):#Calculating Euclidean Distance to determine the nearest neighbors\n",
    "                    ss=ss+pow((data_nor[k][feature[j]] - data_nor[i][feature[j]]),2)\n",
    "                    d=np.sqrt(ss)\n",
    "                if d < dis:\n",
    "                    dis = d\n",
    "                    final = k\n",
    "        if (data_nor[final][0]==data_nor[i][0]):\n",
    "            count += 1\n",
    "            accuracy = (count / (len(data_nor)-1))#Calculating accuracy\n",
    "        else:\n",
    "            error_counter += 1\n",
    "        \n",
    "        if tolerate_error < error_counter:\n",
    "            print(\"Using feature(s), the feature has been stop by minimax like pruning\")\n",
    "            return 0\n",
    "    print(\"Using feature(s)\",feature, \"accuracy is\", round(accuracy*100,2), \"%\")\n",
    "    return accuracy\n",
    "\n",
    "#Performing Leave One Out Cross Validation\n",
    "def CrossValidation(features_in,data_nor,select_f, p):\n",
    "    feature=list(features_in)\n",
    "    if p==1:\n",
    "        feature.append(select_f)\n",
    "    if p ==2:\n",
    "        feature.remove(select_f)\n",
    "    count = 0\n",
    "    dis = np.inf\n",
    "    final=0\n",
    "    for i in range(0,len(data_nor)):\n",
    "        dis = np.inf\n",
    "        for k in range(0,len(data_nor)):\n",
    "            if not np.array_equal(k,i):\n",
    "                ss = 0\n",
    "                for j in range(0, len(feature)):#Calculating Euclidean Distance to determine the nearest neighbors\n",
    "                    ss=ss+pow((data_nor[k][feature[j]] - data_nor[i][feature[j]]),2)\n",
    "                    d=np.sqrt(ss)\n",
    "                if d < dis:\n",
    "                    dis = d\n",
    "                    final = k\n",
    "        if (data_nor[final][0]==data_nor[i][0]):\n",
    "            count += 1\n",
    "            accuracy = (count / (len(data_nor)-1))#Calculating accuracy\n",
    "    print(\"Using feature(s)\",feature, \"accuracy is\", round(accuracy*100,2), \"%\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def ForwardSelection(data_nor,NF):\n",
    "    print(\"Beginning search.\\n\")\n",
    "    current_features = []\n",
    "    final_acc = 0\n",
    "    best_feature=[]\n",
    "    my_windows = [0 for _ in range(NF)]\n",
    "    counter =  0\n",
    "    for i in range(1, NF+1):\n",
    "        print(\"\\n On level %d of the search tree\" % (i),\"contains\", current_features)\n",
    "        feature_select = 0\n",
    "        cur_acc=0.0\n",
    "        for j in range(1, NF+1):\n",
    "            if j not in current_features:\n",
    "                if j == 1:\n",
    "                    acc = CrossValidation(current_features,data_nor,j,1)\n",
    "                else:\n",
    "                    acc = CrossValidation_ML(current_features,data_nor,j,1,(int) ((1-cur_acc) * len(data_nor) ))\n",
    "                if acc> cur_acc:\n",
    "                    cur_acc = acc\n",
    "                    feature_select = j\n",
    "        \n",
    "        current_features.append(feature_select)\n",
    "        print(\"\\n On level %d of the search tree,\" % (i),\"feature %d was added to the current set\" % (feature_select))\n",
    "        print(\"\\n With \", len(current_features), \" features, the accuracy is: \", round(cur_acc * 100,2), \"%\")\n",
    "        \n",
    "        my_windows[counter] = cur_acc        \n",
    "        if counter > 2 and Slide_window(my_windows, counter):\n",
    "            break\n",
    "        counter += 1\n",
    "        if cur_acc >= final_acc: \n",
    "            final_acc= cur_acc\n",
    "            best_feature = list(current_features)\n",
    "\n",
    "    print()\n",
    "    print(\"Finish search!! The best feature subset is:\", best_feature,\"which has an accuracy of\", round(final_acc * 100,2), \"%\")\n",
    "\n",
    "\n",
    "def BackwardElimination(data_nor,NF):\n",
    "    param_stop= input(\"\\nType the accuracy param to early abandon(0-100): \\n You may input 100 if you don't want early abondon \\n \\n\")\n",
    "    param_stop= int(param_stop)\n",
    "    \n",
    "    print(\"Beginning search.\\n\")\n",
    "    final_acc = 0\n",
    "    best_feature=[]\n",
    "    current_features = [i for i in range(1, NF+1)]\n",
    "\n",
    "    for i in range(1, NF):\n",
    "        print(\"\\n On level %d of the search tree\" % (i),\"contains\", current_features)\n",
    "        feature_select = 0\n",
    "        cur_acc = 0\n",
    "        for j in range(1,NF):\n",
    "            if (j in current_features):\n",
    "                acc = CrossValidation(current_features,data_nor,j,2)\n",
    "                if acc > cur_acc:\n",
    "                    cur_acc = acc\n",
    "                    feature_select = j\n",
    "        if feature_select in current_features: \n",
    "            current_features.remove(feature_select) \n",
    "            print(\"\\n On level \", i, \" feature \", feature_select, \" was removed from the current set\")\n",
    "            print(\"\\n With \", len(current_features), \" features, the accuracy is: \", round(cur_acc * 100,2), \"%\")\n",
    "        if cur_acc >= final_acc: \n",
    "            final_acc = cur_acc\n",
    "            best_feature= list(current_features)\n",
    "        if final_acc * 100 > param_stop:\n",
    "            print()\n",
    "            print(\"Early abandon!! The best feature subset is:\", best_feature,\"which has an accuracy of\", round(final_acc * 100,2), \"%\")\n",
    "            return \n",
    "\n",
    "    print()\n",
    "    print(\"Finish search!! The best feature subset is:\", best_feature,\"which has an accuracy of\", round(final_acc * 100,2), \"%\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Welcome to the Feature Selection Algorithm:\")\n",
    "    files = input(\"Type in the name of the file to test: \")\n",
    "    \n",
    "    algorithm=input(\"\\nType the algorithm you want to run:\\n \\n 1.Forward Selection\\n 2.Backward Elimination\\n \\n\")\n",
    "   \n",
    "    data=np.loadtxt(files)\n",
    "#     print(data[1])\n",
    "#     data = data[1000:]\n",
    "    \n",
    "\n",
    "    data = data[:(int)(len(data)*0.9)]#sample the data\n",
    "    N=len(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(N)\n",
    "    data_normal = Normalize_data(data)\n",
    "    NF= len(data_normal[0])-1\n",
    "    print (\"\\nThis dataset has \"+ str(NF)+ \" features (no including the class attribute), with \"+str(N)+ \" instances\")\n",
    "   \n",
    "    if (algorithm == \"1\"):\n",
    "        ForwardSelection(data_normal,NF)\n",
    "    elif(algorithm == \"2\"):\n",
    "        BackwardElimination(data_normal,NF)\n",
    "    else:\n",
    "        print(\"Error input!\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type in the name of the file to test: diabetes.csv\n",
      "nan\n",
      "[[  6.    148.     72.    ...   0.627  50.      1.   ]\n",
      " [  1.     85.     66.    ...   0.351  31.      0.   ]\n",
      " [  8.    183.     64.    ...   0.672  32.      1.   ]\n",
      " ...\n",
      " [  5.    121.     72.    ...   0.245  30.      0.   ]\n",
      " [  1.    126.     60.    ...   0.349  47.      1.   ]\n",
      " [  1.     93.     70.    ...   0.315  23.      0.   ]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "# script for \n",
    "files = input(\"Type in the name of the file to test: \")\n",
    "my_data = genfromtxt(files, delimiter=',')\n",
    "print(my_data[0][0])\n",
    "my_data = np.delete(my_data,0,axis= 0)\n",
    "\n",
    "print(my_data)\n",
    "\n",
    "last_column = my_data[:, -1].reshape(-1, 1) # convert into vector\n",
    "# delete the last\n",
    "other_columns = my_data[:, :-1]\n",
    "# put them into together\n",
    "new_data = np.hstack((last_column, other_columns))\n",
    "np.savetxt('new_data.txt',new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
